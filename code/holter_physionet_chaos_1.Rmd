---
title: "Holter Physionet EDA"
author: "Christine Lucille Kuryla"
date: "2025-03-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)

```

```{r}


folder_path <- here("data_big/physionet_15/is-the-normal-heart-rate-chaotic-1.0.0/")

file_list <- list.files(
  path = folder_path,
  pattern = "nn\\.txt$",
  full.names = TRUE
)

# ------------------------------------------------------------------------
# 3. Read and combine all matching files into a single data frame
#    - set_names() uses each path string as a name so map_dfr(.id = "filename")
#      will store that path in the 'filename' column.
#    - read_table() is used here with col_names = c("col1", "marker", "col2")
#      to match your file structure. Adjust as needed.
#    - mutate(filename_short = basename(filename)) adds the short filename 
#      (e.g. "123_nn.txt" vs. the full path).
# ------------------------------------------------------------------------
combined_df <- file_list %>%
  set_names() %>%  # sets names so ".id" will track the original path
  map_dfr(
    .f = ~ readr::read_table(.x, col_names = c("ibi", "marker", "time_sec")),
    .id = "filename"
  ) %>%
  mutate(filename_short = basename(filename))

# ------------------------------------------------------------------------
# 4. Basic inspection
# ------------------------------------------------------------------------
glimpse(combined_df)
head(combined_df)

# For example, look at how many rows came from each file:
combined_df %>%
  count(filename_short)

# ------------------------------------------------------------------------
# 5. (Optional) If you only want the filename stem (e.g. "123_nn" without .txt),
#    you can use tools::file_path_sans_ext(), or a simple sub() / gsub().
# ------------------------------------------------------------------------
combined_df <- combined_df %>%
  mutate(id = tools::file_path_sans_ext(filename_short)) %>% 
  select(c(id, time_sec, ibi)) %>% 
  mutate(
    condition = case_when(
      str_starts(id, "a") ~ "AF",     # If starts with 'a'
      str_starts(id, "n") ~ "Normal", # If starts with 'n'
      str_starts(id, "c") ~ "CHF",    # If starts with 'c'
      TRUE                          ~ "Unknown"    # For anything else
    )
  ) %>% 
  mutate(
    status = case_when(
      condition %in% c("AF", "Normal") ~ "Healthy",
      condition == "CHF" ~ "CHF",
      TRUE ~ "Unknown"  # or another default if desired
    )
  )
  

data_holter <- combined_df
  
glimpse(data_holter)
  
```


# Calculate downsampled HR

```{r}

# calculate heart rate, rmssd, sdnn every minute

data_holter_minute <- data_holter %>%
  # Ensure data is ordered by time before we take diffs
  arrange(id, time_sec) %>%
  mutate(
    minute_index = floor(time_sec / 60)
  ) %>%
  group_by(id, condition, status, minute_index) %>%
  summarize(
    mean_hr = mean(60 / ibi, na.rm = TRUE),
    # SDNN: standard deviation of the IBIs in that minute
    sdnn = sd(ibi, na.rm = TRUE),
    # RMSSD: square root of mean of squared differences of successive IBI
    rmssd = {
      # We compute consecutive differences of ibi within the group
      diffs <- diff(ibi)
      sqrt(mean(diffs^2, na.rm = TRUE))
    },
    n_beats = n(),
    .groups = "drop"
  ) %>%
  # If desired, a time stamp for each minute
  mutate(minute_start_sec = minute_index * 60) %>%
  arrange(id, minute_index)

# downsample to every 5 minutes
data_holter_downsampled <- data_holter_minute %>%
  filter(minute_index %% 5 == 1) %>% 
  mutate(time_5min = minute_index/5)

# If you need a different offset (e.g., 1, 6, 11, etc.), you can adjust accordingly:
# filter(minute_index %% 5 == 1)

```

# Features

```{r}

library(dplyr)
library(pracma)
library(nonlinearTseries)
library(DescTools)

data_holter_minute_metrics <- data_holter_minute %>%
  group_by(id, condition, status) %>%
  summarize(
    ApEn_HR   = approx_entropy(mean_hr, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    SampEn_HR = sample_entropy(mean_hr, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    ApEn_RMSSD   = approx_entropy(rmssd, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    SampEn_RMSSD = sample_entropy(rmssd, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    ApEn_SDNN   = approx_entropy(sdnn, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    SampEn_SDNN = sample_entropy(sdnn, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    Mean_HR = mean(mean_hr),
    Median_HR = median(mean_hr),
    Max_HR = max(mean_hr),
    Min_HR = min(mean_hr),
    IQR_HR = IQR(mean_hr),
    SD_HR = sd(mean_hr),
    CV_HR   = 100 * SD_HR / Mean_HR,
    Mean_RMSSD = mean(rmssd),
    Median_RMSSD = median(rmssd),
    Max_RMSSD = max(rmssd),
    Min_RMSSD = min(rmssd),
    IQR_RMSSD = IQR(rmssd),
    SD_RMSSD = sd(rmssd),
    CV_RMSSD   = 100 * SD_RMSSD / Mean_RMSSD,
    Mean_SDNN = mean(sdnn),
    Median_SDNN = median(sdnn),
    Max_SDNN = max(sdnn),
    Min_SDNN = min(sdnn),
    IQR_SDNN = IQR(sdnn),
    SD_SDNN = sd(sdnn),
    CV_SDNN   = 100 * SD_SDNN / Mean_SDNN,
    alpha_DFA_HR = {
      # Extract this subject's HR vector
      x <- mean_hr
      x <- x[!is.na(x)]  # remove any NA values
      
      # If there aren't enough points to meaningfully do DFA, return NA
      if (length(x) < 20) {
        NA_real_
      } else {
        # Compute DFA on the vector 'x'
        # Adjust window.size.range and npoints as needed
        dfa_res <- dfa(
          x,
          window.size.range = c(4, floor(length(x) / 4)), 
          npoints = 20,
          doPlot = FALSE
        )
        # The 'slope' in dfa_res is the alpha exponent
        dfa_res$the.slope
      }
    },
  #  PermEn_HR = PermutationEntropy(mean_hr, order = 3), # Adjust 'order' for embedding dimension. Default might be 3 or 4.
    acf_lag1_HR = {
      x <- mean_hr[!is.na(mean_hr)]
      if (length(x) < 2) {
        NA_real_
      } else {
        acf_res <- acf(x, lag.max = 1, plot = FALSE)
        acf_res$acf[2]  # lag 1 correlation
      }
    },
    .groups = "drop"
  )

data_holter_downsampled_metrics <- data_holter_downsampled %>%
  group_by(id, condition, status) %>%
  summarize(
    ApEn_HR   = approx_entropy(mean_hr, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    SampEn_HR = sample_entropy(mean_hr, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    ApEn_RMSSD   = approx_entropy(rmssd, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    SampEn_RMSSD = sample_entropy(rmssd, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    ApEn_SDNN   = approx_entropy(sdnn, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    SampEn_SDNN = sample_entropy(sdnn, edim = 2, r = 0.2 * sd(mean_hr, na.rm = TRUE)),
    Mean_HR = mean(mean_hr),
    Median_HR = median(mean_hr),
    Max_HR = max(mean_hr),
    Min_HR = min(mean_hr),
    IQR_HR = IQR(mean_hr),
    SD_HR = sd(mean_hr),
    CV_HR   = 100 * SD_HR / Mean_HR,
    Mean_RMSSD = mean(rmssd),
    Median_RMSSD = median(rmssd),
    Max_RMSSD = max(rmssd),
    Min_RMSSD = min(rmssd),
    IQR_RMSSD = IQR(rmssd),
    SD_RMSSD = sd(rmssd),
    CV_RMSSD   = 100 * SD_RMSSD / Mean_RMSSD,
    Mean_SDNN = mean(sdnn),
    Median_SDNN = median(sdnn),
    Max_SDNN = max(sdnn),
    Min_SDNN = min(sdnn),
    IQR_SDNN = IQR(sdnn),
    SD_SDNN = sd(sdnn),
    CV_SDNN   = 100 * SD_SDNN / Mean_SDNN,
    alpha_DFA_HR = {
      # Extract this subject's HR vector
      x <- mean_hr
      x <- x[!is.na(x)]  # remove any NA values

      # If there aren't enough points to meaningfully do DFA, return NA
      if (length(x) < 20) {
        NA_real_
      } else {
        # Compute DFA on the vector 'x'
        # Adjust window.size.range and npoints as needed
        dfa_res <- dfa(
          x,
          window.size.range = c(4, floor(length(x) / 4)),
          npoints = 20,
          doPlot = FALSE
        )
        # The 'slope' in dfa_res is the alpha exponent
        dfa_res$the.slope
      }
    },
  #  PermEn_HR = PermutationEntropy(mean_hr, order = 3), # Adjust 'order' for embedding dimension. Default might be 3 or 4.
    acf_lag1_HR = {
      x <- mean_hr[!is.na(mean_hr)]
      if (length(x) < 2) {
        NA_real_
      } else {
        acf_res <- acf(x, lag.max = 1, plot = FALSE)
        acf_res$acf[2]  # lag 1 correlation
      }
    },
  alpha_DFA_HR = {
      x <- mean_hr[!is.na(mean_hr)]
      if (length(x) < 20) {
        NA_real_
      } else {
        # Run dfa(), ignoring the plotting warning
        dfa_res <- suppressWarnings(
          dfa(x, window.size.range = c(4, floor(length(x)/4)),
              npoints = 20, plot = FALSE)
        )
        # Manually compute slope in logâ€“log space
        fit <- lm(log(dfa_res$fluctuation.function) ~ 
                  log(dfa_res$window.sizes))
        coef(fit)[2]  # alpha
      }
    },
    .groups = "drop"
  )


```

# Calculate metrics, include catch 22

```{r}

library(Rcatch22)
library(pracma)

# Function to calculate all metrics for a single variable
calculate_metrics <- function(data_vector) {
  catch22_all(data = data_vector, catch24 = TRUE) %>% 
    bind_rows(
      tibble(
        names = c("mean", "sd", "max", "min", "cv", "iqr", "median", "apen", "sampen"),
        values = c(
          mean(data_vector, na.rm = TRUE),
          sd(data_vector, na.rm = TRUE),
          max(data_vector, na.rm = TRUE),
          min(data_vector, na.rm = TRUE),
          # For coefficient of variation, check mean!=0 to avoid Inf/NaN
          sd(data_vector, na.rm = TRUE) / mean(data_vector, na.rm = TRUE),
          IQR(data_vector, na.rm = TRUE),
          median(data_vector, na.rm = TRUE),
          approx_entropy(data_vector, edim = 2, r = 0.2 * sd(data_vector, na.rm = TRUE)),
          sample_entropy(data_vector, edim = 2, r = 0.2 * sd(data_vector, na.rm = TRUE))
        )
      )
    )
}

# Process data for all three metrics: avg_hr, sdnn, and rmssd
results <- data_holter_downsampled %>%
  mutate(avg_hr = mean_hr) %>% 
  group_by(id, condition, status) %>%
  
  # Create separate list-columns for each metric
  summarize(
    avg_hr_stats = list(calculate_metrics(avg_hr)),
    sdnn_stats = list(calculate_metrics(sdnn)),
    rmssd_stats = list(calculate_metrics(rmssd)),
    .groups = "drop"   # Ungroup after summarizing
  )

# Process avg_hr metrics
avg_hr_results <- results %>%
  select(id, condition, status, avg_hr_stats) %>%
  unnest(cols = avg_hr_stats) %>%
  pivot_wider(
    names_from = names,
    values_from = values,
    names_prefix = "avg_hr_"
  )

# Process sdnn metrics
sdnn_results <- results %>%
  select(id, condition, status, sdnn_stats) %>%
  unnest(cols = sdnn_stats) %>%
  pivot_wider(
    names_from = names,
    values_from = values,
    names_prefix = "sdnn_"
  )

# Process rmssd metrics
rmssd_results <- results %>%
  select(id, condition, status, rmssd_stats) %>%
  unnest(cols = rmssd_stats) %>%
  pivot_wider(
    names_from = names,
    values_from = values,
    names_prefix = "rmssd_"
  )

# Join all results together
final_results <- avg_hr_results %>%
  left_join(sdnn_results, by = c("id", "condition", "status")) %>%
  left_join(rmssd_results, by = c("id", "condition", "status"))

# --- Example check of the resulting data ---
head(final_results)

# --- Sanity checks ---
# Check for missing values in key metrics
colSums(is.na(final_results))

# Check for infinite values
colSums(sapply(final_results, is.infinite))

```




```{r}


# Install packages if you donâ€™t have them
# install.packages(c("dplyr", "tidyr", "purrr", "broom", "ggplot2"))

library(dplyr)
library(tidyr)
library(purrr)
library(broom)
library(ggplot2)

#--------------------------------------------------------------------
# EXAMPLE DATA
#--------------------------------------------------------------------
# Suppose your data looks like this:
# df_metrics <- data.frame(
#   id        = c("a1nn", "a2nn", "a3nn", "b1nn", "b2nn", "b3nn"),
#   status    = c("Healthy", "Healthy", "Healthy", "Unhealthy", "Unhealthy", "Unhealthy"),
#   condition = c("AF", "SR", "AF", "SR", "AF", "VB"),
#   mean_hr   = rnorm(6, 70, 5),
#   sdnn      = rnorm(6, 50, 5),
#   rmssd     = rnorm(6, 40, 5)
# )
#
# In your case, read or construct the real data frame similarly.

df_metrics <- data_holter_downsampled_metrics  # <-- Replace with your actual data
df_metrics <- final_results
df_metrics <- avg_hr_results %>% 
  filter(condition != "AF")

#--------------------------------------------------------------------
# 1. Pivot longer so each metric is in a single column
#--------------------------------------------------------------------
df_long <- df_metrics %>%
  # # Keep only columns that are numeric metrics
  # select(-id, -status, -condition, everything()) %>%  # reorder if you want
  # # The above line ensures we remove id, status, condition, and keep everything else
  # bind_cols(
  #   df_metrics %>% select(id, status, condition)  # re-add these columns
  # ) %>%
  # Reorder columns so that id/status/condition come first
  relocate(id, status, condition) %>%
  pivot_longer(
    cols = -c(id, status, condition),
    names_to = "metric",
    values_to = "value"
  ) %>%
  # Make sure we only keep numeric values, in case thereâ€™s leftover text
  filter(is.numeric(value))

# Alternatively, you could do:
# df_long <- df_metrics %>%
#   select(id, status, condition, where(is.numeric)) %>%
#   pivot_longer(
#     cols = -c(id, status, condition),
#     names_to = "metric",
#     values_to = "value"
#   )

#--------------------------------------------------------------------
# 2. Compare two groups (status) with t-tests or Wilcoxon rank-sum
#--------------------------------------------------------------------

# A. Optional: check normality by Shapiro test or similar within each group
#    If data are large, Shapiro test may be too sensitive â€“ you might rely on a
#    distribution plot or Q-Q plot. For simplicity, let's skip that step or do it
#    in a quick approach:

df_status_tests <- df_long %>%
  group_by(metric) %>%
  group_modify(~ {
    # .x is the data for each metric
    # We'll do a t-test by default; if you want to switch to Wilcoxon based on
    # normality, you can do that here. This example always runs both for illustration.

    data_for_metric <- .x

    # T-test
    ttest_res <- t.test(value ~ status, data = data_for_metric)
    ttest_tidy <- tidy(ttest_res) %>%
      mutate(test = "t-test")

    # Wilcoxon
    wilcox_res <- wilcox.test(value ~ status, data = data_for_metric)
    wilcox_tidy <- tidy(wilcox_res) %>%
      mutate(test = "wilcoxon-rank-sum")

    bind_rows(ttest_tidy, wilcox_tidy)
  }) %>%
  ungroup()

df_status_tests

df_status_tests %>% 
  arrange(p.value)
# -> This will create a data frame with columns: metric, test, estimate, statistic, p.value, conf.low, conf.high, etc.

# B. Boxplot for each metric by status
ggplot(df_long, aes(x = status, y = value)) +
  geom_boxplot() +
  facet_wrap(~ metric, scales = "free_y") +
  theme_minimal() +
  labs(title = "Boxplots of Each Metric by Status")

#--------------------------------------------------------------------
# 3. Compare three groups (condition) with ANOVA or Kruskalâ€“Wallis
#--------------------------------------------------------------------

df_condition_tests <- df_long %>%
  group_by(metric) %>%
  group_modify(~ {
    data_for_metric <- .x

    # One-way ANOVA
    anova_mod <- aov(value ~ condition, data = data_for_metric)
    anova_tidy <- tidy(anova_mod) %>%
      mutate(test = "ANOVA")

    # Kruskalâ€“Wallis
    kruskal_res <- kruskal.test(value ~ condition, data = data_for_metric)
    kruskal_tidy <- tidy(kruskal_res) %>%
      mutate(test = "Kruskalâ€“Wallis")

    bind_rows(anova_tidy, kruskal_tidy)
  }) %>%
  ungroup()

df_condition_tests %>% 
  arrange(p.value)
# -> Similar structure: columns for metric, test, statistic, p.value, etc.

# If ANOVA is significant, you might do pairwise t-tests with corrections:
# pairwise.t.test(df_for_metric$value, df_for_metric$condition, p.adjust.method = "bonferroni")

# C. Boxplots for each metric by condition
ggplot(df_long, aes(x = condition, y = value)) +
  geom_boxplot() +
  facet_wrap(~ metric, scales = "free_y") +
  theme_minimal() +
  labs(title = "Boxplots of Each Metric by Condition")

#--------------------------------------------------------------------
# 4. Interpreting Results
#--------------------------------------------------------------------
# - df_status_tests: results of t-test + Wilcoxon for two-group comparison
# - df_condition_tests: results of ANOVA + Kruskalâ€“Wallis for three-group comparison
# - For significant results, you may want pairwise post-hoc tests (Tukey, Dunn's, etc.).
# - Make sure the assumptions of the tests (normality, variance homogeneity) are reasonably met
#   or that the nonparametric alternatives are used.



```

```{r}

# Install packages if needed:
# install.packages(c("dplyr", "tidyr", "ggplot2", "purrr", "RColorBrewer"))

library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)

#----------------------------------------------------
# 0) Data Setup
#----------------------------------------------------
# Let's assume your data frame is called `df_metrics`,
# with columns: id, status, condition, plus numeric metrics.

# Example skeleton:
# df_metrics <- data.frame(
#   id = ...,
#   status = ...,      # Factor with 2 levels
#   condition = ...,   # Factor with 3 levels
#   mean_hr = ...,     # Numeric
#   sdnn = ...,        # Numeric
#   rmssd = ...        # Numeric
#   # possibly more metrics...
# )

#----------------------------------------------------
# 1) Prepare numeric data for PCA
#----------------------------------------------------
# Remove non-numeric columns and scale the data
df_num <- df_metrics %>%
  select(where(is.numeric)) %>%
  select(where(~ sd(.x, na.rm = TRUE) > 0)) %>% 
  drop_na()  # optional: drop any rows with NAs for simplicity

# Perform PCA
# center = TRUE and scale. = TRUE ensures variables have mean 0, sd 1
pca_res <- prcomp(df_num, center = TRUE, scale. = TRUE)

# Extract the proportion of variance explained
var_explained <- (pca_res$sdev^2) / sum(pca_res$sdev^2)
var_explained_pc <- round(var_explained * 100, 2)  # in percent

#----------------------------------------------------
# 2) Create a heatmap of loadings for PC1 to PC7
#----------------------------------------------------
# pca_res$rotation is a matrix of loadings, rows=variables, cols=PCs
loadings_df <- as.data.frame(pca_res$rotation) %>%
  # Keep only the first 7 PCs (if there are at least 7)
  select(PC1, PC2, PC3, PC4, PC5, PC6, PC7) %>%
  rownames_to_column(var = "variable") %>%
  pivot_longer(
    cols = starts_with("PC"),
    names_to = "PC",
    values_to = "loading"
  )

# We'll make a nicer label for each PC, showing variance explained:
pc_labels <- paste0(
  c("PC1","PC2","PC3","PC4","PC5","PC6","PC7"), "\n(",
  var_explained_pc[1:7], "%)"
)

# Map PC1..PC7 to these labels:
pc_lut <- setNames(pc_labels, paste0("PC", 1:7))

# Heatmap
ggplot(loadings_df, aes(x = PC, y = variable, fill = loading)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  scale_x_discrete(labels = pc_lut) +
  labs(
    title = "Variable Loadings on First 7 PCs",
    x = "Principal Component",
    y = "Variable",
    fill = "Loading"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#----------------------------------------------------
# 3) Make a biplot
#----------------------------------------------------
# We'll do a simple base R biplot for convenience:
biplot(pca_res, scale = 0, cex = 0.6)
# or for a bit more control, you can build a ggplot-based biplot yourself.

#----------------------------------------------------
# 4) PC1 vs PC2 colored by condition, shaped by status
#----------------------------------------------------
# We'll extract the PCA scores (coordinates of each row in df_num)
scores <- as.data.frame(pca_res$x)
scores <- cbind(scores, df_metrics %>% select(id, status, condition))

# Axis labels with variance explained
pc1_lab <- paste0("PC1 (", var_explained_pc[1], "%)")
pc2_lab <- paste0("PC2 (", var_explained_pc[2], "%)")

ggplot(scores, aes(x = PC1, y = PC2,
                   color = condition, shape = status)) +
  geom_point(size = 2) +
  labs(
    title = "PC1 vs PC2",
    x = pc1_lab,
    y = pc2_lab
  ) +
  theme_minimal()


```

# Plot relevant boxplots and statistics (change as needed)
```{r}

library(ggplot2)
library(dplyr)
library(ggpubr)
library(gridExtra)

# Create a plot for each metric
create_plot <- function(metric_name, custom_title = NULL) {
  # Filter data for the specific metric
  plot_data <- df_long %>% filter(metric == metric_name)
  
  # Get the unique status groups
  status_groups <- unique(plot_data$status)
  
  # Get test results
  t_test_results <- df_status_tests %>% 
    filter(metric == metric_name, test == "t-test") %>%
    head(1)
  
  wilcox_results <- df_status_tests %>% 
    filter(metric == metric_name, test == "wilcoxon-rank-sum") %>%
    head(1)
  
  # Calculate means for each group
  means <- plot_data %>% 
    group_by(status) %>% 
    summarize(mean_value = mean(value, na.rm = TRUE))
  
  # Determine y-axis limits
  y_min <- min(plot_data$value, na.rm = TRUE)
  y_max <- max(plot_data$value, na.rm = TRUE)
  y_range <- y_max - y_min
  y_padding <- y_range * 0.1  # Just a little padding for the plot
  
  # Use custom title if provided, otherwise use metric name
  plot_title <- ifelse(!is.null(custom_title), custom_title, metric_name)
  
  # Prepare subtitle with test statistics (will be placed below the plot)
  subtitle <- ""
  
  # Add significance markers based on p-value
  if (nrow(wilcox_results) > 0 && length(status_groups) == 2) {
    sig_level <- ""
    
    if (wilcox_results$p.value < 0.001) sig_level <- "***"
    else if (wilcox_results$p.value < 0.01) sig_level <- "**" 
    else if (wilcox_results$p.value < 0.05) sig_level <- "*"
    
    # Only add significance marker if significant
    if (sig_level != "") {
      # Place significance star at top of plot
      p_mark <- sig_level
    } else {
      p_mark <- ""
    }
  } else {
    p_mark <- ""
  }
  
  # Create the base plot
  p <- ggplot(plot_data, aes(x = status, y = value, fill = status)) +
    geom_boxplot(alpha = 0.7, outlier.size = 2) +
    # Add mean points with labels
    geom_point(data = means, aes(y = mean_value), shape = 18, size = 4, color = "black") +
    geom_text(data = means, aes(y = mean_value, label = round(mean_value, 1)), 
              vjust = -1.5, color = "black", size = 3.5) +
    scale_fill_manual(values = c("CHF" = "#F8AFA8", "Healthy" = "#7FCDBB")) +
    # Add significance marker at top
    annotate("text", x = 1.5, y = y_max + (y_padding * 0.8), 
             label = p_mark, size = 8, hjust = 0.5) +
    labs(
      title = plot_title,
      x = NULL,  # Remove x-axis label as it's redundant
      y = "Value"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
      axis.title.y = element_text(size = 11, margin = margin(r = 10)),
      axis.text = element_text(size = 10),
      legend.position = "none",
      panel.grid.minor = element_blank(),
      plot.margin = margin(b = 0, t = 5, r = 5, l = 5)  # Minimal bottom margin
    ) +
    coord_cartesian(ylim = c(y_min - y_padding, y_max + y_padding * 1.1), clip = "off")
  
  # Create text for below the plot
  plot_text <- NULL
  
  # Get test statistics
  if (nrow(t_test_results) > 0) {
    t_stat <- round(t_test_results$statistic, 2)
    t_p <- formatC(t_test_results$p.value, format = "f", digits = 4)
    
    # Format p-value for display
    if (t_test_results$p.value < 0.0001) {
      t_p_display <- "p<0.0001"
    } else {
      t_p_display <- paste0("p=", t_p)
    }
    
    t_text <- paste0("t-test: stat=", t_stat, ", ", t_p_display)
  } else {
    t_text <- NULL
  }
  
  if (nrow(wilcox_results) > 0) {
    w_stat <- round(wilcox_results$statistic, 0)
    w_p <- formatC(wilcox_results$p.value, format = "f", digits = 4)
    
    # Format p-value for display
    if (wilcox_results$p.value < 0.0001) {
      w_p_display <- "p<0.0001"
    } else {
      w_p_display <- paste0("p=", w_p)
    }
    
    w_text <- paste0("Wilcox: stat=", w_stat, ", ", w_p_display)
  } else {
    w_text <- NULL
  }
  
  # Add comparison of means
  if (!is.na(t_test_results$estimate1[1]) && !is.na(t_test_results$estimate2[1])) {
    group1_mean <- round(t_test_results$estimate1, 1)
    group2_mean <- round(t_test_results$estimate2, 1)
    comp_text <- paste0("(", group1_mean, " vs ", group2_mean, ")")
  } else {
    comp_text <- NULL
  }
  
  # Return the plot and annotation text for below
  return(list(
    plot = p,
    text = list(
      t_test = t_text,
      wilcox = w_text,
      comparison = comp_text
    )
  ))
}

# Custom titles mapping - you can modify these titles
metric_titles <- list(
  "avg_hr_DN_HistogramMode_5" = "HR Histogram Mode (5 Bins)",
  "avg_hr_cv" = "HR Coefficient of Variation",
  "avg_hr_SB_BinaryStats_diff_longstretch0" = "HR Longest Consecutive Decrease", # Longest Run of Zeros in Binary-Differenced Series
  "avg_hr_PD_PeriodicityWang_th0_01" = "HR Periodicity Index (Wang)",
  "avg_hr_IN_AutoMutualInfoStats_40_gaussian_fmmi" = "HR Auto Mutual Information",
  "avg_hr_CO_FirstMin_ac" = "HR First Autocorrelation Minimum"
  # Add more metrics and their custom titles as needed
)

# Comment out automatic metric detection and use the specified list
# metrics_to_plot <- unique(df_long$metric)
metrics_to_plot <- c("avg_hr_DN_HistogramMode_5", "avg_hr_cv", "avg_hr_SB_BinaryStats_diff_longstretch0", 
                     "avg_hr_PD_PeriodicityWang_th0_01", "avg_hr_IN_AutoMutualInfoStats_40_gaussian_fmmi", 
                     "avg_hr_CO_FirstMin_ac")

plot_list <- list()
text_list <- list()

# Create plots with custom titles where available
for (i in seq_along(metrics_to_plot)) {
  metric <- metrics_to_plot[i]
  custom_title <- NULL
  
  # Use custom title if available
  if (metric %in% names(metric_titles)) {
    custom_title <- metric_titles[[metric]]
  }
  
  result <- create_plot(metric, custom_title)
  plot_list[[i]] <- result$plot
  text_list[[i]] <- result$text
}

# Determine a reasonable grid layout based on the number of plots
n_plots <- length(plot_list)
n_cols <- min(3, n_plots)  # Maximum 3 columns
n_rows <- ceiling(n_plots / n_cols)

# Create a combined plot with text below each graph
library(grid)
library(gridExtra)

# Function to create a combined plot with text placed immediately below the graph
create_combined_plot <- function(plot_obj, text_obj, plot_index) {
  # Create text grob with all information
  text_lines <- c()
  
  if (!is.null(text_obj$wilcox)) {
    text_lines <- c(text_lines, text_obj$wilcox)
  }
  
  if (!is.null(text_obj$t_test)) {
    text_lines <- c(text_lines, text_obj$t_test)
  }
  
  if (!is.null(text_obj$comparison)) {
    text_lines <- c(text_lines, text_obj$comparison)
  }
  
  # Join text lines
  text_content <- paste(text_lines, collapse = "\n")
  
  # Create a text grob with larger font size and minimal margins
  text_grob <- textGrob(
    text_content,
    gp = gpar(fontsize = 11),
    just = "center",
    vp = viewport(height = unit(0.9, "npc"))  # Reduces internal padding
  )
  
  # Arrange plot and text with minimal space between
  combined <- arrangeGrob(
    plot_obj,
    text_grob,
    heights = c(4, 0.9),  # Reduced text area height
    padding = unit(0, "line")  # No padding
  )
  
  return(combined)
}

# Create combined plots
combined_plots <- list()
for (i in seq_along(plot_list)) {
  combined_plots[[i]] <- create_combined_plot(plot_list[[i]], text_list[[i]], i)
}

# Layout all plots in a grid with absolutely minimal spacing
grid_plot <- arrangeGrob(
  grobs = combined_plots,
  ncol = n_cols,
  padding = unit(0, "line"),  # No padding at all between panels
  widths = unit(rep(1, n_cols), "null"),
  heights = unit(rep(1, ceiling(n_plots/n_cols)), "null")
)

# Display the plot
grid.newpage()
grid.draw(grid_plot)

# Save the plot with appropriate dimensions (uncomment to use)
# ggsave("metric_comparisons.png", grid_plot, 
#        width = min(n_cols * 3.5, 12),  # Reduced width
#        height = min(n_rows * 3.5, 12),  # Reduced height
#        dpi = 300)

```

